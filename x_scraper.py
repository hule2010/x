#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
X(Twitter) ç”¨æˆ·åæ§½å’Œé—®é¢˜æŠ“å–åˆ†æå·¥å…·
æ”¯æŒä¸­è‹±æ–‡å†…å®¹æŠ“å–ã€æ™ºèƒ½åˆ†ç±»å’Œæœ¬åœ°å­˜å‚¨
é›†æˆSeleniumåçˆ¬æœºåˆ¶ç»•è¿‡åŠŸèƒ½
"""

import tweepy
import pandas as pd
import json
import re
import os
from datetime import datetime, timedelta
import sqlite3
from textblob import TextBlob
import jieba
import jieba.analyse
from collections import Counter
import logging
from typing import List, Dict, Optional
import time
import random

# å¯¼å…¥åçˆ¬åŸºç¡€ç±»
try:
    from selenium_stealth_base import StealthSeleniumBase
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    print("è­¦å‘Š: Seleniumç›¸å…³æ¨¡å—æœªå®‰è£…ï¼Œå°†ä»…ä½¿ç”¨Twitter APIæ¨¡å¼")

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('x_scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class XScraper:
    def __init__(self, config_file: str = 'config.json', use_selenium: bool = False, use_stealth: bool = True):
        """åˆå§‹åŒ–XæŠ“å–å™¨"""
        self.config = self.load_config(config_file)
        self.api = self.setup_twitter_api()
        self.db_path = self.config.get('database_path', 'x_complaints.db')
        self.use_selenium = use_selenium and SELENIUM_AVAILABLE
        self.use_stealth = use_stealth
        self.stealth_driver = None
        self.setup_database()
        
        # è®¾ç½®jiebaåˆ†è¯
        jieba.set_dictionary('dict.txt.big')  # ä½¿ç”¨ç¹ä½“å­—å…¸ä»¥æ”¯æŒæ›´å¤šä¸­æ–‡è¯æ±‡
        
        # åˆå§‹åŒ–Seleniumï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_selenium:
            self.setup_selenium_driver()
        
    def load_config(self, config_file: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.error(f"é…ç½®æ–‡ä»¶ {config_file} æœªæ‰¾åˆ°ï¼Œè¯·å…ˆåˆ›å»ºé…ç½®æ–‡ä»¶")
            return {}
    
    def setup_twitter_api(self) -> Optional[tweepy.API]:
        """è®¾ç½®Twitter API"""
        try:
            # Twitter API v2 å®¢æˆ·ç«¯
            client = tweepy.Client(
                bearer_token=self.config.get('bearer_token'),
                consumer_key=self.config.get('consumer_key'),
                consumer_secret=self.config.get('consumer_secret'),
                access_token=self.config.get('access_token'),
                access_token_secret=self.config.get('access_token_secret'),
                wait_on_rate_limit=True
            )
            
            # éªŒè¯å‡­è¯
            try:
                me = client.get_me()
                logger.info(f"APIè®¤è¯æˆåŠŸï¼Œç”¨æˆ·: {me.data.username}")
                return client
            except Exception as e:
                logger.error(f"APIè®¤è¯å¤±è´¥: {e}")
                return None
                
        except Exception as e:
            logger.error(f"è®¾ç½®Twitter APIå¤±è´¥: {e}")
            return None
    
    def setup_database(self):
        """è®¾ç½®SQLiteæ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºä¸»è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS complaints (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                tweet_id TEXT UNIQUE,
                user_id TEXT,
                username TEXT,
                content TEXT,
                language TEXT,
                created_at TIMESTAMP,
                difficulty_level INTEGER,
                category TEXT,
                keywords TEXT,
                sentiment_score REAL,
                retweet_count INTEGER,
                like_count INTEGER,
                reply_count INTEGER,
                collected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # åˆ›å»ºåˆ†ç±»è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS categories (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT UNIQUE,
                description TEXT,
                keywords TEXT
            )
        ''')
        
        # æ’å…¥é»˜è®¤åˆ†ç±»
        default_categories = [
            ('æŠ€æœ¯é—®é¢˜', 'æŠ€æœ¯ç›¸å…³çš„é—®é¢˜å’Œåæ§½', 'bug,é”™è¯¯,å´©æºƒ,å¡é¡¿,æ…¢,ä¸èƒ½ç”¨,æ•…éšœ,é—®é¢˜'),
            ('ç”¨æˆ·ä½“éªŒ', 'ç•Œé¢å’Œäº¤äº’ä½“éªŒé—®é¢˜', 'UI,ç•Œé¢,ä½“éªŒ,éš¾ç”¨,å¤æ‚,ä¸æ–¹ä¾¿,è®¾è®¡'),
            ('åŠŸèƒ½éœ€æ±‚', 'åŠŸèƒ½ç¼ºå¤±æˆ–æ”¹è¿›å»ºè®®', 'åŠŸèƒ½,éœ€è¦,å¸Œæœ›,å»ºè®®,æ”¹è¿›,å¢åŠ ,ç¼ºå°‘'),
            ('æœåŠ¡è´¨é‡', 'å®¢æœå’ŒæœåŠ¡ç›¸å…³é—®é¢˜', 'å®¢æœ,æœåŠ¡,æ€åº¦,å›å¤,å¤„ç†,è§£å†³'),
            ('ä»·æ ¼è´¹ç”¨', 'ä»·æ ¼å’Œè´¹ç”¨ç›¸å…³æŠ±æ€¨', 'è´µ,ä¾¿å®œ,ä»·æ ¼,è´¹ç”¨,æ”¶è´¹,å…è´¹,ä¼˜æƒ '),
            ('å…¶ä»–', 'å…¶ä»–ç±»å‹çš„åæ§½', 'å…¶ä»–,æ‚é¡¹,ä¸€èˆ¬')
        ]
        
        cursor.executemany(
            'INSERT OR IGNORE INTO categories (name, description, keywords) VALUES (?, ?, ?)',
            default_categories
        )
        
        conn.commit()
        conn.close()
        logger.info("æ•°æ®åº“è®¾ç½®å®Œæˆ")
    
    def setup_selenium_driver(self):
        """è®¾ç½®Seleniumé©±åŠ¨ï¼ˆå¸¦åçˆ¬åŠŸèƒ½ï¼‰"""
        if not SELENIUM_AVAILABLE:
            logger.warning("Seleniumä¸å¯ç”¨ï¼Œè·³è¿‡é©±åŠ¨è®¾ç½®")
            return False
        
        try:
            if self.use_stealth:
                logger.info("åˆå§‹åŒ–åçˆ¬Seleniumé©±åŠ¨")
                self.stealth_driver = StealthSeleniumBase(
                    headless=True,
                    use_undetected=True,
                    use_stealth=True,
                    use_proxy=self.config.get('proxy', None),
                    window_size=(1920, 1080)
                )
                logger.info("åçˆ¬Seleniumé©±åŠ¨åˆå§‹åŒ–æˆåŠŸ")
                return True
            else:
                logger.info("åˆå§‹åŒ–æ ‡å‡†Seleniumé©±åŠ¨")
                # è¿™é‡Œå¯ä»¥æ·»åŠ æ ‡å‡†Seleniumåˆå§‹åŒ–ä»£ç 
                return True
                
        except Exception as e:
            logger.error(f"Seleniumé©±åŠ¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.use_selenium = False
            return False
    
    def selenium_search_tweets(self, query: str, max_results: int = 100) -> List[Dict]:
        """ä½¿ç”¨Seleniumæœç´¢æ¨æ–‡ï¼ˆåçˆ¬ç‰ˆæœ¬ï¼‰"""
        if not self.use_selenium or not self.stealth_driver:
            logger.warning("Seleniumæœªå¯ç”¨ï¼Œä½¿ç”¨APIæœç´¢")
            return self.search_complaints(query, max_results)
        
        tweets = []
        try:
            # æ„å»ºæœç´¢URL
            search_url = f"https://twitter.com/search?q={query}&src=typed_query&f=live"
            logger.info(f"ä½¿ç”¨Seleniumæœç´¢: {query}")
            
            # è®¿é—®æœç´¢é¡µé¢
            if not self.stealth_driver.get_page(search_url):
                logger.error("æ— æ³•è®¿é—®Twitteræœç´¢é¡µé¢")
                return tweets
            
            # ç­‰å¾…æ¨æ–‡åŠ è½½
            time.sleep(3)
            
            # æ»šåŠ¨åŠ è½½æ›´å¤šæ¨æ–‡
            for scroll_count in range(5):
                # æ¨¡æ‹Ÿäººç±»æ»šåŠ¨è¡Œä¸º
                self.stealth_driver.simulate_human_behavior()
                
                # æ»šåŠ¨é¡µé¢
                scroll_distance = random.randint(800, 1200)
                self.stealth_driver.execute_script(f"window.scrollTo(0, {scroll_distance * (scroll_count + 1)});")
                
                # éšæœºç­‰å¾…
                time.sleep(random.uniform(2, 4))
            
            # æå–æ¨æ–‡
            tweet_elements = self.stealth_driver.driver.find_elements(By.CSS_SELECTOR, '[data-testid="tweet"]')
            logger.info(f"æ‰¾åˆ° {len(tweet_elements)} ä¸ªæ¨æ–‡å…ƒç´ ")
            
            for i, tweet_element in enumerate(tweet_elements[:max_results]):
                try:
                    tweet_data = self.extract_tweet_from_element(tweet_element, query)
                    if tweet_data:
                        tweets.append(tweet_data)
                        
                except Exception as e:
                    logger.warning(f"æå–ç¬¬ {i+1} ä¸ªæ¨æ–‡å¤±è´¥: {e}")
                    continue
            
            logger.info(f"Seleniumæœç´¢å®Œæˆï¼Œè·å¾— {len(tweets)} æ¡æ¨æ–‡")
            
        except Exception as e:
            logger.error(f"Seleniumæœç´¢å‡ºé”™: {e}")
        
        return tweets
    
    def extract_tweet_from_element(self, tweet_element, search_query: str) -> Optional[Dict]:
        """ä»æ¨æ–‡å…ƒç´ æå–æ•°æ®"""
        try:
            # æå–æ¨æ–‡æ–‡æœ¬
            try:
                text_element = tweet_element.find_element(By.CSS_SELECTOR, '[data-testid="tweetText"]')
                content = text_element.text
            except:
                content = ""
            
            if not content:
                return None
            
            # æå–ç”¨æˆ·ä¿¡æ¯
            try:
                user_element = tweet_element.find_element(By.CSS_SELECTOR, '[data-testid="User-Name"] a')
                username = user_element.get_attribute('href').split('/')[-1]
            except:
                username = "unknown"
            
            # æå–äº’åŠ¨æ•°æ®
            like_count = self.extract_interaction_count(tweet_element, '[data-testid="like"]')
            retweet_count = self.extract_interaction_count(tweet_element, '[data-testid="retweet"]')
            reply_count = self.extract_interaction_count(tweet_element, '[data-testid="reply"]')
            
            # ç”Ÿæˆå”¯ä¸€ID
            tweet_id = f"selenium_{username}_{hash(content)}_{int(time.time())}"
            
            # åˆ†ææ¨æ–‡
            language = self.detect_language(content)
            keywords = self.extract_keywords(content, language)
            difficulty = self.calculate_difficulty_level(content, language)
            category = self.categorize_complaint(content, keywords)
            sentiment = self.calculate_sentiment_score(content, language)
            
            return {
                'tweet_id': tweet_id,
                'user_id': username,
                'username': username,
                'content': content,
                'language': language,
                'created_at': datetime.now(),
                'difficulty_level': difficulty,
                'category': category,
                'keywords': ','.join(keywords),
                'sentiment_score': sentiment,
                'retweet_count': retweet_count,
                'like_count': like_count,
                'reply_count': reply_count
            }
            
        except Exception as e:
            logger.warning(f"æå–æ¨æ–‡æ•°æ®å¤±è´¥: {e}")
            return None
    
    def extract_interaction_count(self, tweet_element, selector: str) -> int:
        """æå–äº’åŠ¨æ•°é‡"""
        try:
            count_element = tweet_element.find_element(By.CSS_SELECTOR, selector)
            count_text = count_element.text.strip()
            
            if not count_text or count_text == '0':
                return 0
            
            # å¤„ç†K, Mç­‰å•ä½
            if 'K' in count_text.upper():
                return int(float(count_text.upper().replace('K', '')) * 1000)
            elif 'M' in count_text.upper():
                return int(float(count_text.upper().replace('M', '')) * 1000000)
            else:
                # æå–æ•°å­—
                numbers = re.findall(r'\d+', count_text)
                return int(numbers[0]) if numbers else 0
                
        except:
            return 0
    
    def detect_language(self, text: str) -> str:
        """æ£€æµ‹æ–‡æœ¬è¯­è¨€"""
        # ç®€å•çš„è¯­è¨€æ£€æµ‹
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        
        if chinese_chars > english_chars:
            return 'zh'
        elif english_chars > 0:
            return 'en'
        else:
            return 'unknown'
    
    def extract_keywords(self, text: str, language: str) -> List[str]:
        """æå–å…³é”®è¯"""
        if language == 'zh':
            # ä¸­æ–‡å…³é”®è¯æå–
            keywords = jieba.analyse.extract_tags(text, topK=10, withWeight=False)
        else:
            # è‹±æ–‡å…³é”®è¯æå–
            blob = TextBlob(text.lower())
            words = [word for word in blob.words if len(word) > 2]
            word_freq = Counter(words)
            keywords = [word for word, freq in word_freq.most_common(10)]
        
        return keywords
    
    def calculate_difficulty_level(self, text: str, language: str) -> int:
        """è®¡ç®—é—®é¢˜éš¾æ˜“ç¨‹åº¦ (1-5çº§)"""
        difficulty_score = 1
        
        # åŸºäºå…³é”®è¯åˆ¤æ–­éš¾åº¦
        high_difficulty_keywords = {
            'zh': ['ç³»ç»Ÿ', 'æ•°æ®åº“', 'æœåŠ¡å™¨', 'ç½‘ç»œ', 'ç®—æ³•', 'æ¶æ„', 'é›†æˆ', 'å…¼å®¹æ€§', 'å®‰å…¨', 'æ€§èƒ½'],
            'en': ['system', 'database', 'server', 'network', 'algorithm', 'architecture', 'integration', 'compatibility', 'security', 'performance']
        }
        
        medium_difficulty_keywords = {
            'zh': ['åŠŸèƒ½', 'è®¾ç½®', 'é…ç½®', 'å¯¼å…¥', 'å¯¼å‡º', 'åŒæ­¥', 'å¤‡ä»½'],
            'en': ['feature', 'setting', 'configuration', 'import', 'export', 'sync', 'backup']
        }
        
        low_difficulty_keywords = {
            'zh': ['ç•Œé¢', 'é¢œè‰²', 'å­—ä½“', 'å¸ƒå±€', 'æ˜¾ç¤º'],
            'en': ['interface', 'color', 'font', 'layout', 'display']
        }
        
        text_lower = text.lower()
        
        # æ£€æŸ¥é«˜éš¾åº¦å…³é”®è¯
        high_count = sum(1 for keyword in high_difficulty_keywords.get(language, []) if keyword in text_lower)
        medium_count = sum(1 for keyword in medium_difficulty_keywords.get(language, []) if keyword in text_lower)
        low_count = sum(1 for keyword in low_difficulty_keywords.get(language, []) if keyword in text_lower)
        
        if high_count >= 2:
            difficulty_score = 5
        elif high_count >= 1:
            difficulty_score = 4
        elif medium_count >= 2:
            difficulty_score = 3
        elif medium_count >= 1 or low_count >= 2:
            difficulty_score = 2
        else:
            difficulty_score = 1
        
        # åŸºäºæ–‡æœ¬é•¿åº¦è°ƒæ•´
        if len(text) > 200:
            difficulty_score = min(5, difficulty_score + 1)
        
        return difficulty_score
    
    def categorize_complaint(self, text: str, keywords: List[str]) -> str:
        """å¯¹æŠ•è¯‰è¿›è¡Œåˆ†ç±»"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('SELECT name, keywords FROM categories')
        categories = cursor.fetchall()
        conn.close()
        
        text_lower = text.lower()
        category_scores = {}
        
        for category_name, category_keywords in categories:
            score = 0
            category_keyword_list = category_keywords.split(',')
            
            for keyword in category_keyword_list:
                if keyword.strip().lower() in text_lower:
                    score += 2
            
            # æ£€æŸ¥æå–çš„å…³é”®è¯
            for keyword in keywords:
                if keyword.lower() in [ck.strip().lower() for ck in category_keyword_list]:
                    score += 1
            
            category_scores[category_name] = score
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„åˆ†ç±»
        if category_scores:
            best_category = max(category_scores, key=category_scores.get)
            if category_scores[best_category] > 0:
                return best_category
        
        return 'å…¶ä»–'
    
    def calculate_sentiment_score(self, text: str, language: str) -> float:
        """è®¡ç®—æƒ…æ„Ÿåˆ†æ•°"""
        if language == 'en':
            blob = TextBlob(text)
            return blob.sentiment.polarity
        else:
            # ç®€å•çš„ä¸­æ–‡æƒ…æ„Ÿåˆ†æ
            negative_words = ['ä¸å¥½', 'å·®', 'çƒ‚', 'åƒåœ¾', 'è®¨åŒ', 'å¤±æœ›', 'ç³Ÿç³•', 'é—®é¢˜', 'é”™è¯¯', 'æ•…éšœ']
            positive_words = ['å¥½', 'æ£’', 'ä¼˜ç§€', 'æ»¡æ„', 'å–œæ¬¢', 'ä¸é”™', 'å®Œç¾']
            
            negative_count = sum(1 for word in negative_words if word in text)
            positive_count = sum(1 for word in positive_words if word in text)
            
            if negative_count + positive_count == 0:
                return 0.0
            
            return (positive_count - negative_count) / (positive_count + negative_count)
    
    def search_complaints(self, query: str, max_results: int = 100, use_selenium: bool = None) -> List[Dict]:
        """æœç´¢æŠ±æ€¨å’Œé—®é¢˜ç›¸å…³çš„æ¨æ–‡"""
        # å†³å®šä½¿ç”¨å“ªç§æ–¹å¼
        if use_selenium is None:
            use_selenium = self.use_selenium
        
        if use_selenium and self.stealth_driver:
            logger.info("ä½¿ç”¨Seleniumåçˆ¬æ¨¡å¼æœç´¢")
            return self.selenium_search_tweets(query, max_results)
        elif self.api:
            logger.info("ä½¿ç”¨Twitter APIæœç´¢")
            return self.api_search_complaints(query, max_results)
        else:
            logger.error("Twitter APIå’ŒSeleniuméƒ½æœªå¯ç”¨")
            return []
    
    def api_search_complaints(self, query: str, max_results: int = 100) -> List[Dict]:
        """ä½¿ç”¨Twitter APIæœç´¢æŠ±æ€¨å’Œé—®é¢˜ç›¸å…³çš„æ¨æ–‡"""
        if not self.api:
            logger.error("Twitter APIæœªåˆå§‹åŒ–")
            return []
        
        complaints = []
        
        try:
            # æ„å»ºæœç´¢æŸ¥è¯¢
            complaint_keywords = [
                "é—®é¢˜", "bug", "é”™è¯¯", "æ•…éšœ", "ä¸èƒ½ç”¨", "å´©æºƒ", "å¡é¡¿",
                "problem", "issue", "error", "broken", "crash", "slow",
                "å·®è¯„", "åæ§½", "æŠ±æ€¨", "ä¸æ»¡", "å¤±æœ›",
                "complaint", "disappointed", "frustrated", "terrible"
            ]
            
            search_query = f"{query} ({' OR '.join(complaint_keywords)}) -is:retweet lang:zh OR lang:en"
            
            # æœç´¢æ¨æ–‡
            tweets = tweepy.Paginator(
                self.api.search_recent_tweets,
                query=search_query,
                max_results=min(max_results, 100),
                tweet_fields=['created_at', 'author_id', 'public_metrics', 'lang']
            ).flatten(limit=max_results)
            
            for tweet in tweets:
                if not tweet.text:
                    continue
                
                # è¿‡æ»¤è½¬å‘å’Œå›å¤
                if tweet.text.startswith('RT @') or tweet.text.startswith('@'):
                    continue
                
                language = self.detect_language(tweet.text)
                keywords = self.extract_keywords(tweet.text, language)
                difficulty = self.calculate_difficulty_level(tweet.text, language)
                category = self.categorize_complaint(tweet.text, keywords)
                sentiment = self.calculate_sentiment_score(tweet.text, language)
                
                complaint_data = {
                    'tweet_id': tweet.id,
                    'user_id': tweet.author_id,
                    'content': tweet.text,
                    'language': language,
                    'created_at': tweet.created_at,
                    'difficulty_level': difficulty,
                    'category': category,
                    'keywords': ','.join(keywords),
                    'sentiment_score': sentiment,
                    'retweet_count': tweet.public_metrics['retweet_count'] if tweet.public_metrics else 0,
                    'like_count': tweet.public_metrics['like_count'] if tweet.public_metrics else 0,
                    'reply_count': tweet.public_metrics['reply_count'] if tweet.public_metrics else 0
                }
                
                complaints.append(complaint_data)
                
                # æ·»åŠ å»¶è¿Ÿä»¥é¿å…APIé™åˆ¶
                time.sleep(random.uniform(0.5, 1.5))
                
        except Exception as e:
            logger.error(f"æœç´¢æ¨æ–‡æ—¶å‡ºé”™: {e}")
        
        logger.info(f"æ‰¾åˆ° {len(complaints)} æ¡ç›¸å…³æ¨æ–‡")
        return complaints
    
    def save_complaints(self, complaints: List[Dict]):
        """ä¿å­˜æŠ±æ€¨æ•°æ®åˆ°æ•°æ®åº“"""
        if not complaints:
            return
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for complaint in complaints:
            try:
                cursor.execute('''
                    INSERT OR REPLACE INTO complaints 
                    (tweet_id, user_id, username, content, language, created_at, 
                     difficulty_level, category, keywords, sentiment_score, 
                     retweet_count, like_count, reply_count)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    complaint['tweet_id'],
                    complaint['user_id'],
                    complaint.get('username', ''),
                    complaint['content'],
                    complaint['language'],
                    complaint['created_at'],
                    complaint['difficulty_level'],
                    complaint['category'],
                    complaint['keywords'],
                    complaint['sentiment_score'],
                    complaint['retweet_count'],
                    complaint['like_count'],
                    complaint['reply_count']
                ))
            except Exception as e:
                logger.error(f"ä¿å­˜æ•°æ®æ—¶å‡ºé”™: {e}")
        
        conn.commit()
        conn.close()
        logger.info(f"æˆåŠŸä¿å­˜ {len(complaints)} æ¡æ•°æ®")
    
    def export_to_files(self, output_dir: str = 'output'):
        """å¯¼å‡ºæ•°æ®åˆ°æ–‡ä»¶"""
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        conn = sqlite3.connect(self.db_path)
        
        # å¯¼å‡ºæ‰€æœ‰æ•°æ®
        df_all = pd.read_sql_query('SELECT * FROM complaints ORDER BY created_at DESC', conn)
        df_all.to_csv(f'{output_dir}/all_complaints.csv', index=False, encoding='utf-8')
        df_all.to_json(f'{output_dir}/all_complaints.json', orient='records', ensure_ascii=False, indent=2)
        
        # æŒ‰æ—¥æœŸåˆ†ç»„å¯¼å‡º
        df_all['date'] = pd.to_datetime(df_all['created_at']).dt.date
        for date in df_all['date'].unique():
            date_df = df_all[df_all['date'] == date]
            date_str = str(date)
            date_df.to_csv(f'{output_dir}/complaints_{date_str}.csv', index=False, encoding='utf-8')
        
        # æŒ‰éš¾åº¦ç­‰çº§å¯¼å‡º
        for level in range(1, 6):
            level_df = df_all[df_all['difficulty_level'] == level]
            if not level_df.empty:
                level_df.to_csv(f'{output_dir}/complaints_level_{level}.csv', index=False, encoding='utf-8')
        
        # æŒ‰åˆ†ç±»å¯¼å‡º
        for category in df_all['category'].unique():
            if pd.isna(category):
                continue
            cat_df = df_all[df_all['category'] == category]
            safe_category = re.sub(r'[^\w\-_\.]', '_', category)
            cat_df.to_csv(f'{output_dir}/complaints_{safe_category}.csv', index=False, encoding='utf-8')
        
        conn.close()
        logger.info(f"æ•°æ®å·²å¯¼å‡ºåˆ° {output_dir} ç›®å½•")
    
    def generate_report(self) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        conn = sqlite3.connect(self.db_path)
        
        # åŸºæœ¬ç»Ÿè®¡
        total_count = pd.read_sql_query('SELECT COUNT(*) as count FROM complaints', conn).iloc[0]['count']
        
        # æŒ‰è¯­è¨€ç»Ÿè®¡
        lang_stats = pd.read_sql_query('''
            SELECT language, COUNT(*) as count 
            FROM complaints 
            GROUP BY language 
            ORDER BY count DESC
        ''', conn)
        
        # æŒ‰éš¾åº¦ç­‰çº§ç»Ÿè®¡
        difficulty_stats = pd.read_sql_query('''
            SELECT difficulty_level, COUNT(*) as count 
            FROM complaints 
            GROUP BY difficulty_level 
            ORDER BY difficulty_level
        ''', conn)
        
        # æŒ‰åˆ†ç±»ç»Ÿè®¡
        category_stats = pd.read_sql_query('''
            SELECT category, COUNT(*) as count 
            FROM complaints 
            GROUP BY category 
            ORDER BY count DESC
        ''', conn)
        
        # æŒ‰æ—¥æœŸç»Ÿè®¡
        daily_stats = pd.read_sql_query('''
            SELECT DATE(created_at) as date, COUNT(*) as count 
            FROM complaints 
            GROUP BY DATE(created_at) 
            ORDER BY date DESC 
            LIMIT 7
        ''', conn)
        
        # æƒ…æ„Ÿåˆ†æç»Ÿè®¡
        sentiment_stats = pd.read_sql_query('''
            SELECT 
                AVG(sentiment_score) as avg_sentiment,
                MIN(sentiment_score) as min_sentiment,
                MAX(sentiment_score) as max_sentiment
            FROM complaints
        ''', conn)
        
        conn.close()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""
# X(Twitter) ç”¨æˆ·åæ§½åˆ†ææŠ¥å‘Š

## æ€»ä½“ç»Ÿè®¡
- æ€»æŠ“å–æ•°é‡: {total_count} æ¡
- ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## è¯­è¨€åˆ†å¸ƒ
{lang_stats.to_string(index=False)}

## éš¾åº¦ç­‰çº§åˆ†å¸ƒ
{difficulty_stats.to_string(index=False)}

## åˆ†ç±»ç»Ÿè®¡
{category_stats.to_string(index=False)}

## æœ€è¿‘7å¤©æ•°æ®é‡
{daily_stats.to_string(index=False)}

## æƒ…æ„Ÿåˆ†æ
- å¹³å‡æƒ…æ„Ÿåˆ†æ•°: {sentiment_stats.iloc[0]['avg_sentiment']:.3f}
- æœ€è´Ÿé¢åˆ†æ•°: {sentiment_stats.iloc[0]['min_sentiment']:.3f}
- æœ€æ­£é¢åˆ†æ•°: {sentiment_stats.iloc[0]['max_sentiment']:.3f}

æ³¨ï¼šæƒ…æ„Ÿåˆ†æ•°èŒƒå›´ -1.0 (æœ€è´Ÿé¢) åˆ° 1.0 (æœ€æ­£é¢)
"""
        
        return report

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='X(Twitter) ç”¨æˆ·åæ§½æŠ“å–å·¥å…·')
    parser.add_argument('--selenium', action='store_true', help='ä½¿ç”¨Seleniumåçˆ¬æ¨¡å¼')
    parser.add_argument('--stealth', action='store_true', default=True, help='å¯ç”¨åæ£€æµ‹åŠŸèƒ½')
    parser.add_argument('--headless', action='store_true', default=True, help='æ— å¤´æ¨¡å¼')
    parser.add_argument('--proxy', type=str, help='ä»£ç†æœåŠ¡å™¨ (host:port)')
    parser.add_argument('--max-results', type=int, default=50, help='æ¯ä¸ªæŸ¥è¯¢çš„æœ€å¤§ç»“æœæ•°')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæŠ“å–å™¨
    scraper = XScraper(
        config_file='config.json',
        use_selenium=args.selenium,
        use_stealth=args.stealth
    )
    
    print("ğŸš€ X(Twitter) ç”¨æˆ·åæ§½æŠ“å–å·¥å…·")
    print("=" * 50)
    
    if args.selenium and SELENIUM_AVAILABLE:
        print("âœ“ ä½¿ç”¨Seleniumåçˆ¬æ¨¡å¼")
        if args.stealth:
            print("âœ“ å¯ç”¨åæ£€æµ‹åŠŸèƒ½")
    elif scraper.api:
        print("âœ“ ä½¿ç”¨Twitter APIæ¨¡å¼")
    else:
        print("âœ— æ— å¯ç”¨çš„æŠ“å–æ–¹å¼")
        return
    
    # ç¤ºä¾‹æœç´¢æŸ¥è¯¢
    queries = [
        "å¾®ä¿¡é—®é¢˜",
        "æ”¯ä»˜å®bug", 
        "æ·˜å®è´­ç‰©é—®é¢˜",
        "iPhoneæ•…éšœ",
        "Androidå¡é¡¿",
        "Windowsé”™è¯¯"
    ]
    
    all_complaints = []
    
    try:
        for query in queries:
            logger.info(f"æœç´¢å…³é”®è¯: {query}")
            complaints = scraper.search_complaints(query, max_results=args.max_results)
            all_complaints.extend(complaints)
            
            # å»¶è¿Ÿï¼ˆSeleniumæ¨¡å¼å»¶è¿Ÿæ›´é•¿ï¼‰
            delay = random.uniform(5, 10) if args.selenium else random.uniform(2, 5)
            logger.info(f"ç­‰å¾… {delay:.1f} ç§’...")
            time.sleep(delay)
        
        # ä¿å­˜æ•°æ®
        if all_complaints:
            scraper.save_complaints(all_complaints)
            
            # å¯¼å‡ºæ–‡ä»¶
            scraper.export_to_files()
            
            # ç”ŸæˆæŠ¥å‘Š
            report = scraper.generate_report()
            with open('output/analysis_report.md', 'w', encoding='utf-8') as f:
                f.write(report)
            
            print(f"\nğŸ‰ æŠ“å–å®Œæˆï¼")
            print(f"ğŸ“Š å…±è·å¾— {len(all_complaints)} æ¡æ•°æ®")
            print(f"ğŸ“ ç»“æœä¿å­˜åœ¨ output ç›®å½•")
            print(report)
            logger.info("æŠ“å–å’Œåˆ†æå®Œæˆï¼")
        else:
            logger.warning("æœªæ‰¾åˆ°ç›¸å…³æ•°æ®")
            
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æŠ“å–")
    except Exception as e:
        logger.error(f"æŠ“å–è¿‡ç¨‹å‡ºé”™: {e}")
    finally:
        # æ¸…ç†èµ„æº
        if scraper.stealth_driver:
            scraper.stealth_driver.quit()
            logger.info("Seleniumé©±åŠ¨å·²å…³é—­")

if __name__ == "__main__":
    main()