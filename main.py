#!/usr/bin/env python3
"""
X(Twitter)åæ§½æŠ“å–å’Œåˆ†æç³»ç»Ÿä¸»ç¨‹åº
"""

import argparse
import logging
import sys
from datetime import datetime, timedelta
from typing import List, Dict
import json

from src.twitter_scraper import TwitterScraper
from src.text_analyzer import TextAnalyzer
from src.data_manager import DataManager
import config

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('twitter_complaints.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class ComplaintAnalysisSystem:
    def __init__(self, use_api=True):
        self.scraper = TwitterScraper(use_api=use_api)
        self.analyzer = TextAnalyzer()
        self.data_manager = DataManager(config.DATA_CONFIG['path'])
        
    def run_analysis(self, keywords: List[str] = None, 
                    max_tweets: int = None,
                    days_back: int = None) -> Dict[str, str]:
        """è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹"""
        # ä½¿ç”¨é»˜è®¤å€¼
        if not keywords:
            keywords = config.SEARCH_CONFIG['default_keywords']
        if not max_tweets:
            max_tweets = config.DATA_CONFIG['max_tweets_per_search']
        if not days_back:
            days_back = config.DATA_CONFIG['days_back']
        
        logger.info(f"Starting analysis for keywords: {keywords}")
        logger.info(f"Parameters: max_tweets={max_tweets}, days_back={days_back}")
        
        try:
            # 1. æŠ“å–æ¨æ–‡
            logger.info("Step 1: Scraping tweets...")
            raw_tweets = self.scraper.search_complaints(
                keywords=keywords,
                max_results=max_tweets,
                days_back=days_back
            )
            logger.info(f"Scraped {len(raw_tweets)} tweets")
            
            if not raw_tweets:
                logger.warning("No tweets found. Exiting.")
                return {}
            
            # ä¿å­˜åŸå§‹æ•°æ®
            raw_file = self.data_manager.save_raw_tweets(raw_tweets)
            
            # 2. åˆ†ææ¨æ–‡
            logger.info("Step 2: Analyzing tweets...")
            analyzed_tweets = []
            
            for i, tweet in enumerate(raw_tweets):
                if i % 10 == 0:
                    logger.info(f"Analyzing tweet {i+1}/{len(raw_tweets)}")
                
                try:
                    analysis = self.analyzer.analyze_tweet(tweet)
                    analyzed_tweets.append(analysis)
                except Exception as e:
                    logger.error(f"Error analyzing tweet {i+1}: {e}")
                    continue
            
            logger.info(f"Successfully analyzed {len(analyzed_tweets)} tweets")
            
            # 3. ä¿å­˜åˆ†æç»“æœ
            logger.info("Step 3: Saving analysis results...")
            saved_files = self.data_manager.save_analyzed_data(analyzed_tweets)
            
            # 4. ç”Ÿæˆç»Ÿè®¡æ‘˜è¦
            summary = self._generate_summary(analyzed_tweets)
            
            logger.info("Analysis completed successfully!")
            
            return {
                'raw_file': raw_file,
                'saved_files': saved_files,
                'summary': summary
            }
            
        except Exception as e:
            logger.error(f"Error during analysis: {e}")
            raise
        finally:
            self.scraper.close()
    
    def _generate_summary(self, analyzed_tweets: List[Dict]) -> Dict:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        total = len(analyzed_tweets)
        complaints = [t for t in analyzed_tweets if t.get('is_complaint', False)]
        
        summary = {
            'total_tweets_analyzed': total,
            'total_complaints': len(complaints),
            'complaint_rate': len(complaints) / total if total > 0 else 0,
            'languages': {},
            'sentiments': {},
            'difficulties': {},
            'categories': {},
            'keywords': []
        }
        
        # ç»Ÿè®¡å„ç»´åº¦æ•°æ®
        for tweet in analyzed_tweets:
            # è¯­è¨€
            lang = tweet.get('language', 'unknown')
            summary['languages'][lang] = summary['languages'].get(lang, 0) + 1
            
            # æƒ…æ„Ÿ
            sentiment = tweet['sentiment']['sentiment']
            summary['sentiments'][sentiment] = summary['sentiments'].get(sentiment, 0) + 1
            
            # éš¾åº¦
            difficulty = tweet['difficulty']['level']
            summary['difficulties'][difficulty] = summary['difficulties'].get(difficulty, 0) + 1
            
            # ç±»åˆ«
            for category in tweet.get('categories', []):
                summary['categories'][category] = summary['categories'].get(category, 0) + 1
        
        # æå–å…³é”®è¯
        if complaints:
            complaint_texts = [t['original_tweet']['text'] for t in complaints]
            keywords = self.analyzer.extract_keywords(complaint_texts, top_n=15)
            summary['keywords'] = keywords
        
        return summary
    
    def search_and_display(self, keywords: List[str] = None):
        """æœç´¢å¹¶æ˜¾ç¤ºç»“æœï¼ˆäº¤äº’å¼ï¼‰"""
        results = self.run_analysis(keywords)
        
        if results and 'summary' in results:
            summary = results['summary']
            
            print("\n" + "="*60)
            print("ğŸ“Š åˆ†ææ‘˜è¦ | Analysis Summary")
            print("="*60)
            
            print(f"\næ€»æ¨æ–‡æ•° | Total tweets: {summary['total_tweets_analyzed']}")
            print(f"åæ§½æ•°é‡ | Complaints: {summary['total_complaints']} ({summary['complaint_rate']:.1%})")
            
            print(f"\nè¯­è¨€åˆ†å¸ƒ | Language Distribution:")
            for lang, count in summary['languages'].items():
                lang_name = "ä¸­æ–‡" if lang == 'zh' else "English" if lang == 'en' else lang
                print(f"  - {lang_name}: {count}")
            
            print(f"\næƒ…æ„Ÿåˆ†å¸ƒ | Sentiment Distribution:")
            for sentiment, count in summary['sentiments'].items():
                emoji = "ğŸ˜Š" if sentiment == 'positive' else "ğŸ˜" if sentiment == 'neutral' else "ğŸ˜”"
                print(f"  {emoji} {sentiment}: {count}")
            
            print(f"\néš¾åº¦åˆ†å¸ƒ | Difficulty Distribution:")
            for difficulty, count in summary['difficulties'].items():
                level = "ğŸŸ¢ ç®€å•" if difficulty == 'easy' else "ğŸŸ¡ ä¸­ç­‰" if difficulty == 'medium' else "ğŸ”´ å›°éš¾"
                print(f"  {level} ({difficulty}): {count}")
            
            print(f"\né—®é¢˜ç±»åˆ« | Problem Categories:")
            sorted_categories = sorted(summary['categories'].items(), key=lambda x: x[1], reverse=True)
            for category, count in sorted_categories[:5]:
                print(f"  - {category}: {count}")
            
            if summary['keywords']:
                print(f"\nçƒ­é—¨å…³é”®è¯ | Top Keywords:")
                print(f"  {', '.join(summary['keywords'][:10])}")
            
            print(f"\nğŸ“ æ•°æ®å·²ä¿å­˜è‡³ | Data saved to: {config.DATA_CONFIG['path']}/")
            print("="*60)


def main():
    parser = argparse.ArgumentParser(
        description='X(Twitter)åæ§½æŠ“å–å’Œåˆ†æç³»ç»Ÿ | Twitter Complaint Analysis System'
    )
    
    parser.add_argument(
        '-k', '--keywords',
        nargs='+',
        help='æœç´¢å…³é”®è¯ | Keywords to search (space-separated)'
    )
    
    parser.add_argument(
        '-n', '--max-tweets',
        type=int,
        default=100,
        help='æ¯ä¸ªå…³é”®è¯æœ€å¤§æŠ“å–æ•°é‡ | Maximum tweets per keyword (default: 100)'
    )
    
    parser.add_argument(
        '-d', '--days',
        type=int,
        default=7,
        help='æœç´¢è¿‡å»Nå¤©çš„æ•°æ® | Search tweets from past N days (default: 7)'
    )
    
    parser.add_argument(
        '--no-api',
        action='store_true',
        help='ä¸ä½¿ç”¨APIï¼Œä½¿ç”¨ç½‘é¡µçˆ¬è™« | Use web scraping instead of API'
    )
    
    parser.add_argument(
        '--analyze-existing',
        type=str,
        help='åˆ†æå·²å­˜åœ¨çš„åŸå§‹æ•°æ®æ–‡ä»¶ | Analyze existing raw data file'
    )
    
    args = parser.parse_args()
    
    try:
        system = ComplaintAnalysisSystem(use_api=not args.no_api)
        
        if args.analyze_existing:
            # åˆ†æå·²æœ‰æ•°æ®
            logger.info(f"Analyzing existing file: {args.analyze_existing}")
            # TODO: å®ç°åˆ†æå·²æœ‰æ•°æ®çš„åŠŸèƒ½
        else:
            # æ‰§è¡Œæ–°çš„æœç´¢å’Œåˆ†æ
            system.search_and_display(
                keywords=args.keywords,
                max_tweets=args.max_tweets,
                days_back=args.days
            )
            
    except KeyboardInterrupt:
        logger.info("\nAnalysis interrupted by user")
        sys.exit(0)
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()