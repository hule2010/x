#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµç¨‹ä¼˜åŒ–å•†æœºå‘ç°å·¥å…· - åŸºäºSeleniumçš„X(Twitter)æŠ“å–å™¨
æ”¯æŒMacå’ŒWindowsç³»ç»Ÿï¼Œä¸“é—¨æŠ“å–æµç¨‹ä¼˜åŒ–ç›¸å…³çš„ç”¨æˆ·æŠ±æ€¨
"""

import os
import sys
import time
import random
import json
import sqlite3
import pandas as pd
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import platform
import subprocess
import requests
import zipfile
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('process_scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ProcessOptimizationScraper:
    def __init__(self):
        self.system = platform.system().lower()
        self.driver = None
        self.db_path = 'process_opportunities.db'
        self.setup_database()
        
        # æµç¨‹ä¼˜åŒ–ç›¸å…³å…³é”®è¯
        self.process_keywords = {
            'zh': [
                'æµç¨‹', 'æ­¥éª¤', 'æ‰‹ç»­', 'åŠç†', 'æ’é˜Ÿ', 'ç­‰å¾…', 'å®¡æ‰¹', 'ç”³è¯·',
                'ç¹ç', 'å¤æ‚', 'éº»çƒ¦', 'æ…¢', 'æ•ˆç‡', 'è€—æ—¶', 'æµªè´¹æ—¶é—´',
                'æ¥å›è·‘', 'è·‘è…¿', 'é‡å¤', 'å¤šæ¬¡', 'åå¤', 'æŠ˜è…¾',
                'å®¢æœ', 'äººå·¥', 'è½¬æ¥', 'ç­‰å®¢æœ', 'æ‰“ä¸é€š',
                'ç³»ç»Ÿ', 'ç½‘ç«™', 'APP', 'å¡é¡¿', 'å´©æºƒ', 'ç™»å½•',
                'é“¶è¡Œ', 'æ”¿åŠ¡', 'åŒ»é™¢', 'å­¦æ ¡', 'å¿«é€’', 'å¤–å–',
                'é€€è´§', 'é€€æ¬¾', 'æ¢è´§', 'å”®å', 'ç»´ä¿®'
            ],
            'en': [
                'process', 'procedure', 'workflow', 'steps', 'queue', 'wait', 'waiting',
                'approval', 'application', 'form', 'paperwork', 'bureaucracy',
                'complicated', 'complex', 'confusing', 'slow', 'inefficient', 'waste',
                'time-consuming', 'lengthy', 'tedious', 'frustrating',
                'customer service', 'support', 'call center', 'hold', 'transfer',
                'system', 'website', 'app', 'crash', 'login', 'error',
                'bank', 'government', 'hospital', 'school', 'delivery', 'shipping',
                'return', 'refund', 'exchange', 'repair', 'maintenance'
            ]
        }
        
        # æµç¨‹ç—›ç‚¹åˆ†ç±»
        self.pain_point_categories = {
            'æ•ˆç‡é—®é¢˜': ['æ…¢', 'ç­‰å¾…', 'è€—æ—¶', 'slow', 'wait', 'time-consuming'],
            'å¤æ‚åº¦é—®é¢˜': ['å¤æ‚', 'ç¹ç', 'éº»çƒ¦', 'complex', 'complicated', 'confusing'],
            'é‡å¤æ“ä½œ': ['é‡å¤', 'åå¤', 'å¤šæ¬¡', 'repeat', 'multiple', 'again'],
            'ç³»ç»ŸæŠ€æœ¯': ['ç³»ç»Ÿ', 'ç½‘ç«™', 'APP', 'system', 'website', 'app'],
            'äººå·¥æœåŠ¡': ['å®¢æœ', 'äººå·¥', 'ç”µè¯', 'customer service', 'support', 'call'],
            'æµç¨‹è®¾è®¡': ['æµç¨‹', 'æ­¥éª¤', 'æ‰‹ç»­', 'process', 'procedure', 'workflow'],
            'æˆæœ¬é—®é¢˜': ['è´µ', 'è´¹ç”¨', 'æ”¶è´¹', 'expensive', 'cost', 'fee'],
            'ä½“éªŒé—®é¢˜': ['ä½“éªŒ', 'ä¸ä¾¿', 'éº»çƒ¦', 'experience', 'inconvenient', 'hassle']
        }
    
    def setup_database(self):
        """è®¾ç½®æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºæµç¨‹ä¼˜åŒ–æœºä¼šè¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS process_complaints (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                tweet_id TEXT UNIQUE,
                user_handle TEXT,
                content TEXT,
                language TEXT,
                created_at TIMESTAMP,
                pain_point_category TEXT,
                opportunity_score INTEGER,
                business_sector TEXT,
                process_type TEXT,
                optimization_potential TEXT,
                keywords TEXT,
                like_count INTEGER,
                retweet_count INTEGER,
                reply_count INTEGER,
                collected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # åˆ›å»ºå•†æœºåˆ†æè¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS business_opportunities (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                category TEXT,
                description TEXT,
                frequency INTEGER,
                avg_opportunity_score REAL,
                potential_solution TEXT,
                market_size_estimate TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
        logger.info("æ•°æ®åº“è®¾ç½®å®Œæˆ")
    
    def get_chrome_driver_path(self):
        """æ ¹æ®ç³»ç»Ÿç±»å‹è·å–Chromeé©±åŠ¨è·¯å¾„"""
        if self.system == 'darwin':  # macOS
            return './drivers/mac/chromedriver'
        elif self.system == 'windows':
            return './drivers/windows/chromedriver.exe'
        else:  # Linux
            return './drivers/linux/chromedriver'
    
    def download_chromedriver(self):
        """è‡ªåŠ¨ä¸‹è½½ChromeDriver"""
        drivers_dir = './drivers'
        
        if self.system == 'darwin':
            system_dir = 'mac'
            driver_url = 'https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_mac64.zip'
            driver_name = 'chromedriver'
        elif self.system == 'windows':
            system_dir = 'windows'
            driver_url = 'https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_win32.zip'
            driver_name = 'chromedriver.exe'
        else:
            system_dir = 'linux'
            driver_url = 'https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_linux64.zip'
            driver_name = 'chromedriver'
        
        system_drivers_dir = os.path.join(drivers_dir, system_dir)
        driver_path = os.path.join(system_drivers_dir, driver_name)
        
        # å¦‚æœé©±åŠ¨å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
        if os.path.exists(driver_path):
            return driver_path
        
        # åˆ›å»ºç›®å½•
        os.makedirs(system_drivers_dir, exist_ok=True)
        
        try:
            logger.info(f"æ­£åœ¨ä¸‹è½½ {self.system} ç³»ç»Ÿçš„ChromeDriver...")
            response = requests.get(driver_url)
            zip_path = os.path.join(system_drivers_dir, 'chromedriver.zip')
            
            with open(zip_path, 'wb') as f:
                f.write(response.content)
            
            # è§£å‹
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(system_drivers_dir)
            
            # åˆ é™¤zipæ–‡ä»¶
            os.remove(zip_path)
            
            # ç»™é©±åŠ¨æ–‡ä»¶æ‰§è¡Œæƒé™ (Mac/Linux)
            if self.system in ['darwin', 'linux']:
                os.chmod(driver_path, 0o755)
            
            logger.info(f"ChromeDriverä¸‹è½½å®Œæˆ: {driver_path}")
            return driver_path
            
        except Exception as e:
            logger.error(f"ä¸‹è½½ChromeDriverå¤±è´¥: {e}")
            return None
    
    def setup_chrome_driver(self, headless=True):
        """è®¾ç½®Chromeæµè§ˆå™¨é©±åŠ¨"""
        try:
            # è·å–æˆ–ä¸‹è½½é©±åŠ¨
            driver_path = self.download_chromedriver()
            if not driver_path:
                logger.error("æ— æ³•è·å–ChromeDriver")
                return None
            
            # Chromeé€‰é¡¹
            chrome_options = Options()
            if headless:
                chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36')
            
            # åˆ›å»ºæœåŠ¡
            service = Service(driver_path)
            
            # åˆ›å»ºé©±åŠ¨
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            logger.info("Chromeé©±åŠ¨è®¾ç½®æˆåŠŸ")
            return self.driver
            
        except Exception as e:
            logger.error(f"è®¾ç½®Chromeé©±åŠ¨å¤±è´¥: {e}")
            return None
    
    def search_process_complaints(self, search_terms, max_tweets=50):
        """æœç´¢æµç¨‹ç›¸å…³æŠ±æ€¨"""
        if not self.driver:
            logger.error("æµè§ˆå™¨é©±åŠ¨æœªåˆå§‹åŒ–")
            return []
        
        complaints = []
        
        for term in search_terms:
            try:
                # æ„å»ºæœç´¢URL
                search_query = f"{term} (æµç¨‹ OR æ­¥éª¤ OR éº»çƒ¦ OR å¤æ‚ OR process OR procedure OR complicated)"
                search_url = f"https://twitter.com/search?q={search_query}&src=typed_query&f=live"
                
                logger.info(f"æœç´¢å…³é”®è¯: {term}")
                self.driver.get(search_url)
                
                # ç­‰å¾…é¡µé¢åŠ è½½
                time.sleep(3)
                
                # æ»šåŠ¨åŠ è½½æ›´å¤šæ¨æ–‡
                for _ in range(5):  # æ»šåŠ¨5æ¬¡
                    self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(2)
                
                # æŸ¥æ‰¾æ¨æ–‡å…ƒç´ 
                tweets = self.driver.find_elements(By.CSS_SELECTOR, '[data-testid="tweet"]')
                
                for tweet in tweets[:max_tweets]:
                    try:
                        complaint_data = self.extract_tweet_data(tweet, term)
                        if complaint_data and self.is_process_related(complaint_data['content']):
                            complaints.append(complaint_data)
                    except Exception as e:
                        logger.warning(f"æå–æ¨æ–‡æ•°æ®å¤±è´¥: {e}")
                        continue
                
                # éšæœºå»¶è¿Ÿé¿å…è¢«æ£€æµ‹
                time.sleep(random.uniform(2, 5))
                
            except Exception as e:
                logger.error(f"æœç´¢ {term} æ—¶å‡ºé”™: {e}")
                continue
        
        logger.info(f"å…±æ‰¾åˆ° {len(complaints)} æ¡æµç¨‹ç›¸å…³æŠ±æ€¨")
        return complaints
    
    def extract_tweet_data(self, tweet_element, search_term):
        """æå–æ¨æ–‡æ•°æ®"""
        try:
            # æå–æ¨æ–‡å†…å®¹
            content_element = tweet_element.find_element(By.CSS_SELECTOR, '[data-testid="tweetText"]')
            content = content_element.text if content_element else ""
            
            # æå–ç”¨æˆ·ä¿¡æ¯
            try:
                user_element = tweet_element.find_element(By.CSS_SELECTOR, '[data-testid="User-Name"] a')
                user_handle = user_element.get_attribute('href').split('/')[-1] if user_element else ""
            except:
                user_handle = ""
            
            # æå–äº’åŠ¨æ•°æ®
            like_count = self.extract_count(tweet_element, '[data-testid="like"]')
            retweet_count = self.extract_count(tweet_element, '[data-testid="retweet"]')
            reply_count = self.extract_count(tweet_element, '[data-testid="reply"]')
            
            # ç”Ÿæˆå”¯ä¸€ID
            tweet_id = f"{user_handle}_{hash(content)}_{int(time.time())}"
            
            # åˆ†æå†…å®¹
            language = self.detect_language(content)
            pain_category = self.categorize_pain_point(content)
            opportunity_score = self.calculate_opportunity_score(content, like_count, retweet_count)
            business_sector = self.identify_business_sector(content)
            process_type = self.identify_process_type(content)
            
            return {
                'tweet_id': tweet_id,
                'user_handle': user_handle,
                'content': content,
                'language': language,
                'created_at': datetime.now(),
                'pain_point_category': pain_category,
                'opportunity_score': opportunity_score,
                'business_sector': business_sector,
                'process_type': process_type,
                'optimization_potential': self.assess_optimization_potential(content),
                'keywords': search_term,
                'like_count': like_count,
                'retweet_count': retweet_count,
                'reply_count': reply_count
            }
            
        except Exception as e:
            logger.warning(f"æå–æ¨æ–‡æ•°æ®æ—¶å‡ºé”™: {e}")
            return None
    
    def extract_count(self, tweet_element, selector):
        """æå–äº’åŠ¨æ•°é‡"""
        try:
            count_element = tweet_element.find_element(By.CSS_SELECTOR, selector)
            count_text = count_element.text
            if count_text:
                # å¤„ç†K, Mç­‰å•ä½
                if 'K' in count_text:
                    return int(float(count_text.replace('K', '')) * 1000)
                elif 'M' in count_text:
                    return int(float(count_text.replace('M', '')) * 1000000)
                else:
                    return int(count_text) if count_text.isdigit() else 0
            return 0
        except:
            return 0
    
    def detect_language(self, text):
        """æ£€æµ‹è¯­è¨€"""
        chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
        english_chars = len([c for c in text if c.isalpha() and ord(c) < 128])
        
        if chinese_chars > english_chars:
            return 'zh'
        elif english_chars > 0:
            return 'en'
        return 'unknown'
    
    def is_process_related(self, content):
        """åˆ¤æ–­æ˜¯å¦ä¸æµç¨‹ç›¸å…³"""
        content_lower = content.lower()
        
        # æ£€æŸ¥ä¸­è‹±æ–‡å…³é”®è¯
        for lang in ['zh', 'en']:
            for keyword in self.process_keywords[lang]:
                if keyword in content_lower:
                    return True
        
        return False
    
    def categorize_pain_point(self, content):
        """åˆ†ç±»ç—›ç‚¹"""
        content_lower = content.lower()
        category_scores = {}
        
        for category, keywords in self.pain_point_categories.items():
            score = 0
            for keyword in keywords:
                if keyword in content_lower:
                    score += 1
            category_scores[category] = score
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„åˆ†ç±»
        if category_scores:
            return max(category_scores, key=category_scores.get)
        return 'å…¶ä»–'
    
    def calculate_opportunity_score(self, content, like_count, retweet_count):
        """è®¡ç®—å•†æœºåˆ†æ•° (1-10)"""
        score = 1
        
        # åŸºäºäº’åŠ¨æ•°é‡
        total_engagement = like_count + retweet_count * 2
        if total_engagement > 100:
            score += 3
        elif total_engagement > 50:
            score += 2
        elif total_engagement > 10:
            score += 1
        
        # åŸºäºç—›ç‚¹å¼ºåº¦å…³é”®è¯
        pain_intensity_keywords = {
            'zh': ['å¤ª', 'éå¸¸', 'æå…¶', 'è¶…çº§', 'ç‰¹åˆ«', 'çœŸçš„', 'å®Œå…¨', 'æ ¹æœ¬'],
            'en': ['very', 'extremely', 'super', 'really', 'totally', 'completely', 'absolutely']
        }
        
        content_lower = content.lower()
        for lang_keywords in pain_intensity_keywords.values():
            for keyword in lang_keywords:
                if keyword in content_lower:
                    score += 1
                    break
        
        # åŸºäºé¢‘æ¬¡è¯æ±‡
        frequency_keywords = {
            'zh': ['æ¯æ¬¡', 'æ€»æ˜¯', 'ç»å¸¸', 'è€æ˜¯', 'ä¸€ç›´', 'åå¤'],
            'en': ['every time', 'always', 'often', 'constantly', 'repeatedly']
        }
        
        for lang_keywords in frequency_keywords.values():
            for keyword in lang_keywords:
                if keyword in content_lower:
                    score += 2
                    break
        
        return min(score, 10)  # æœ€é«˜10åˆ†
    
    def identify_business_sector(self, content):
        """è¯†åˆ«ä¸šåŠ¡é¢†åŸŸ"""
        content_lower = content.lower()
        
        sectors = {
            'é‡‘èæœåŠ¡': ['é“¶è¡Œ', 'æ”¯ä»˜', 'è´·æ¬¾', 'ä¿¡ç”¨å¡', 'bank', 'payment', 'loan', 'credit'],
            'ç”µå•†è´­ç‰©': ['æ·˜å®', 'äº¬ä¸œ', 'è´­ç‰©', 'ä¸‹å•', 'shopping', 'order', 'amazon', 'ebay'],
            'æ”¿åŠ¡æœåŠ¡': ['æ”¿åºœ', 'åŠè¯', 'æˆ·ç±', 'ç¤¾ä¿', 'government', 'license', 'permit'],
            'åŒ»ç–—å¥åº·': ['åŒ»é™¢', 'æŒ‚å·', 'çœ‹ç—…', 'åŒ»ä¿', 'hospital', 'appointment', 'medical'],
            'æ•™è‚²åŸ¹è®­': ['å­¦æ ¡', 'æŠ¥å', 'è€ƒè¯•', 'å­¦ç±', 'school', 'enrollment', 'exam'],
            'ç‰©æµå¿«é€’': ['å¿«é€’', 'ç‰©æµ', 'é…é€', 'åŒ…è£¹', 'delivery', 'shipping', 'package'],
            'é¤é¥®å¤–å–': ['å¤–å–', 'ç‚¹é¤', 'é…é€', 'food delivery', 'takeout', 'restaurant'],
            'äº¤é€šå‡ºè¡Œ': ['æ‰“è½¦', 'åœ°é“', 'å…¬äº¤', 'åœè½¦', 'taxi', 'subway', 'parking'],
            'æˆ¿äº§ç§Ÿèµ': ['ç§Ÿæˆ¿', 'ä¹°æˆ¿', 'ä¸­ä»‹', 'rent', 'real estate', 'property'],
            'å®¢æˆ·æœåŠ¡': ['å®¢æœ', 'å”®å', 'æŠ•è¯‰', 'customer service', 'support', 'complaint']
        }
        
        for sector, keywords in sectors.items():
            for keyword in keywords:
                if keyword in content_lower:
                    return sector
        
        return 'å…¶ä»–'
    
    def identify_process_type(self, content):
        """è¯†åˆ«æµç¨‹ç±»å‹"""
        content_lower = content.lower()
        
        process_types = {
            'æ³¨å†Œç™»å½•': ['æ³¨å†Œ', 'ç™»å½•', 'éªŒè¯', 'register', 'login', 'verification'],
            'ç”³è¯·å®¡æ‰¹': ['ç”³è¯·', 'å®¡æ‰¹', 'å®¡æ ¸', 'application', 'approval', 'review'],
            'æ”¯ä»˜ç»“ç®—': ['æ”¯ä»˜', 'ä»˜æ¬¾', 'ç»“ç®—', 'payment', 'checkout', 'billing'],
            'é€€æ¢è´§': ['é€€è´§', 'é€€æ¬¾', 'æ¢è´§', 'return', 'refund', 'exchange'],
            'å®¢æˆ·æœåŠ¡': ['å®¢æœ', 'å’¨è¯¢', 'æŠ•è¯‰', 'customer service', 'inquiry', 'complaint'],
            'é¢„çº¦æ’é˜Ÿ': ['é¢„çº¦', 'æ’é˜Ÿ', 'ç­‰å·', 'appointment', 'queue', 'waiting'],
            'ä¿¡æ¯æŸ¥è¯¢': ['æŸ¥è¯¢', 'æœç´¢', 'æŸ¥çœ‹', 'search', 'query', 'check'],
            'æ•°æ®å½•å…¥': ['å¡«å†™', 'å½•å…¥', 'æäº¤', 'fill', 'input', 'submit']
        }
        
        for process_type, keywords in process_types.items():
            for keyword in keywords:
                if keyword in content_lower:
                    return process_type
        
        return 'å…¶ä»–'
    
    def assess_optimization_potential(self, content):
        """è¯„ä¼°ä¼˜åŒ–æ½œåŠ›"""
        content_lower = content.lower()
        
        # é«˜ä¼˜åŒ–æ½œåŠ›æŒ‡æ ‡
        high_potential = ['è‡ªåŠ¨åŒ–', 'ç®€åŒ–', 'ä¸€é”®', 'ç›´æ¥', 'automation', 'simplify', 'direct']
        medium_potential = ['æ”¹è¿›', 'ä¼˜åŒ–', 'æå‡', 'improve', 'optimize', 'enhance']
        low_potential = ['ä¹ æƒ¯', 'æ¥å—', 'ç†è§£', 'used to', 'accept', 'understand']
        
        for keyword in high_potential:
            if keyword in content_lower:
                return 'é«˜'
        
        for keyword in medium_potential:
            if keyword in content_lower:
                return 'ä¸­'
        
        for keyword in low_potential:
            if keyword in content_lower:
                return 'ä½'
        
        # é»˜è®¤åŸºäºç—›ç‚¹å¼ºåº¦åˆ¤æ–­
        pain_words = ['å¤ª', 'éå¸¸', 'æå…¶', 'very', 'extremely', 'terrible']
        for word in pain_words:
            if word in content_lower:
                return 'é«˜'
        
        return 'ä¸­'
    
    def save_complaints(self, complaints):
        """ä¿å­˜æŠ±æ€¨æ•°æ®"""
        if not complaints:
            return
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for complaint in complaints:
            try:
                cursor.execute('''
                    INSERT OR REPLACE INTO process_complaints 
                    (tweet_id, user_handle, content, language, created_at, 
                     pain_point_category, opportunity_score, business_sector, 
                     process_type, optimization_potential, keywords, 
                     like_count, retweet_count, reply_count)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    complaint['tweet_id'],
                    complaint['user_handle'],
                    complaint['content'],
                    complaint['language'],
                    complaint['created_at'],
                    complaint['pain_point_category'],
                    complaint['opportunity_score'],
                    complaint['business_sector'],
                    complaint['process_type'],
                    complaint['optimization_potential'],
                    complaint['keywords'],
                    complaint['like_count'],
                    complaint['retweet_count'],
                    complaint['reply_count']
                ))
            except Exception as e:
                logger.error(f"ä¿å­˜æ•°æ®æ—¶å‡ºé”™: {e}")
        
        conn.commit()
        conn.close()
        logger.info(f"æˆåŠŸä¿å­˜ {len(complaints)} æ¡æ•°æ®")
    
    def analyze_business_opportunities(self):
        """åˆ†æå•†ä¸šæœºä¼š"""
        conn = sqlite3.connect(self.db_path)
        
        # æŒ‰ç—›ç‚¹åˆ†ç±»ç»Ÿè®¡
        pain_point_analysis = pd.read_sql_query('''
            SELECT 
                pain_point_category,
                COUNT(*) as frequency,
                AVG(opportunity_score) as avg_score,
                business_sector,
                process_type,
                optimization_potential
            FROM process_complaints 
            GROUP BY pain_point_category, business_sector, process_type
            ORDER BY frequency DESC, avg_score DESC
        ''', conn)
        
        # ç”Ÿæˆå•†æœºå»ºè®®
        opportunities = []
        for _, row in pain_point_analysis.iterrows():
            if row['frequency'] >= 3 and row['avg_score'] >= 5:  # é«˜é¢‘é«˜åˆ†çš„ç—›ç‚¹
                opportunity = {
                    'category': row['pain_point_category'],
                    'description': f"{row['business_sector']}é¢†åŸŸçš„{row['process_type']}æµç¨‹ä¼˜åŒ–",
                    'frequency': row['frequency'],
                    'avg_opportunity_score': row['avg_score'],
                    'potential_solution': self.generate_solution_suggestion(row),
                    'market_size_estimate': self.estimate_market_size(row['frequency'], row['avg_score'])
                }
                opportunities.append(opportunity)
        
        # ä¿å­˜å•†æœºåˆ†æ
        cursor = conn.cursor()
        cursor.execute('DELETE FROM business_opportunities')  # æ¸…é™¤æ—§æ•°æ®
        
        for opp in opportunities:
            cursor.execute('''
                INSERT INTO business_opportunities 
                (category, description, frequency, avg_opportunity_score, 
                 potential_solution, market_size_estimate)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                opp['category'],
                opp['description'],
                opp['frequency'],
                opp['avg_opportunity_score'],
                opp['potential_solution'],
                opp['market_size_estimate']
            ))
        
        conn.commit()
        conn.close()
        
        logger.info(f"è¯†åˆ«å‡º {len(opportunities)} ä¸ªå•†ä¸šæœºä¼š")
        return opportunities
    
    def generate_solution_suggestion(self, row):
        """ç”Ÿæˆè§£å†³æ–¹æ¡ˆå»ºè®®"""
        category = row['pain_point_category']
        sector = row['business_sector']
        process_type = row['process_type']
        
        solution_templates = {
            'æ•ˆç‡é—®é¢˜': f"å¼€å‘{sector}é¢†åŸŸçš„{process_type}è‡ªåŠ¨åŒ–å·¥å…·ï¼Œå‡å°‘ç­‰å¾…æ—¶é—´",
            'å¤æ‚åº¦é—®é¢˜': f"è®¾è®¡ç®€åŒ–ç‰ˆ{process_type}æµç¨‹ï¼Œé™ä½{sector}è¡Œä¸šç”¨æˆ·çš„æ“ä½œå¤æ‚åº¦",
            'é‡å¤æ“ä½œ': f"åˆ›å»º{process_type}ä¸€é”®å¼è§£å†³æ–¹æ¡ˆï¼Œæ¶ˆé™¤{sector}ä¸­çš„é‡å¤æ­¥éª¤",
            'ç³»ç»ŸæŠ€æœ¯': f"ä¼˜åŒ–{sector}çš„{process_type}ç³»ç»Ÿæ¶æ„ï¼Œæå‡ç¨³å®šæ€§å’Œç”¨æˆ·ä½“éªŒ",
            'äººå·¥æœåŠ¡': f"å¼•å…¥AIå®¢æœç³»ç»Ÿï¼Œæ”¹å–„{sector}çš„{process_type}äººå·¥æœåŠ¡æ•ˆç‡",
            'æµç¨‹è®¾è®¡': f"é‡æ–°è®¾è®¡{sector}çš„{process_type}æµç¨‹ï¼Œé‡‡ç”¨æ›´ç›´è§‚çš„ç”¨æˆ·ç•Œé¢",
            'æˆæœ¬é—®é¢˜': f"æä¾›{sector}{process_type}çš„ä½æˆæœ¬æ›¿ä»£æ–¹æ¡ˆæˆ–å…è´¹å·¥å…·",
            'ä½“éªŒé—®é¢˜': f"å¼€å‘ä¸“æ³¨äº{sector}{process_type}ç”¨æˆ·ä½“éªŒçš„è§£å†³æ–¹æ¡ˆ"
        }
        
        return solution_templates.get(category, f"ä¼˜åŒ–{sector}çš„{process_type}æµç¨‹")
    
    def estimate_market_size(self, frequency, avg_score):
        """ä¼°ç®—å¸‚åœºè§„æ¨¡"""
        if frequency >= 20 and avg_score >= 8:
            return "å¤§å‹å¸‚åœº (>1000ä¸‡ç”¨æˆ·)"
        elif frequency >= 10 and avg_score >= 6:
            return "ä¸­å‹å¸‚åœº (100ä¸‡-1000ä¸‡ç”¨æˆ·)"
        elif frequency >= 5 and avg_score >= 5:
            return "å°å‹å¸‚åœº (10ä¸‡-100ä¸‡ç”¨æˆ·)"
        else:
            return "ç»†åˆ†å¸‚åœº (<10ä¸‡ç”¨æˆ·)"
    
    def export_results(self):
        """å¯¼å‡ºç»“æœ"""
        output_dir = 'process_optimization_output'
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        conn = sqlite3.connect(self.db_path)
        
        # å¯¼å‡ºåŸå§‹æ•°æ®
        complaints_df = pd.read_sql_query('SELECT * FROM process_complaints ORDER BY opportunity_score DESC', conn)
        complaints_df.to_csv(f'{output_dir}/process_complaints.csv', index=False, encoding='utf-8')
        
        # å¯¼å‡ºå•†æœºåˆ†æ
        opportunities_df = pd.read_sql_query('SELECT * FROM business_opportunities ORDER BY avg_opportunity_score DESC', conn)
        opportunities_df.to_csv(f'{output_dir}/business_opportunities.csv', index=False, encoding='utf-8')
        
        conn.close()
        
        logger.info(f"ç»“æœå·²å¯¼å‡ºåˆ° {output_dir} ç›®å½•")
    
    def generate_report(self):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        conn = sqlite3.connect(self.db_path)
        
        # åŸºç¡€ç»Ÿè®¡
        total_complaints = pd.read_sql_query('SELECT COUNT(*) as count FROM process_complaints', conn).iloc[0]['count']
        opportunities = pd.read_sql_query('SELECT * FROM business_opportunities ORDER BY avg_opportunity_score DESC', conn)
        
        # ç—›ç‚¹åˆ†ç±»ç»Ÿè®¡
        pain_stats = pd.read_sql_query('''
            SELECT pain_point_category, COUNT(*) as count, AVG(opportunity_score) as avg_score
            FROM process_complaints 
            GROUP BY pain_point_category 
            ORDER BY count DESC
        ''', conn)
        
        # ä¸šåŠ¡é¢†åŸŸç»Ÿè®¡
        sector_stats = pd.read_sql_query('''
            SELECT business_sector, COUNT(*) as count, AVG(opportunity_score) as avg_score
            FROM process_complaints 
            GROUP BY business_sector 
            ORDER BY count DESC
        ''', conn)
        
        conn.close()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""# æµç¨‹ä¼˜åŒ–å•†æœºå‘ç°åˆ†ææŠ¥å‘Š

## æŠ¥å‘Šç”Ÿæˆæ—¶é—´
{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## æ•°æ®æ¦‚è§ˆ
- **æ€»æŠ“å–æ•°é‡**: {total_complaints} æ¡æµç¨‹ç›¸å…³æŠ±æ€¨
- **è¯†åˆ«å•†æœºæ•°é‡**: {len(opportunities)} ä¸ª
- **å¹³å‡å•†æœºåˆ†æ•°**: {opportunities['avg_opportunity_score'].mean():.2f}/10

## ç—›ç‚¹åˆ†ç±»åˆ†æ
"""
        
        for _, row in pain_stats.iterrows():
            report += f"- **{row['pain_point_category']}**: {row['count']} æ¡ (å¹³å‡åˆ†æ•°: {row['avg_score']:.2f})\n"
        
        report += f"""
## ä¸šåŠ¡é¢†åŸŸåˆ†æ
"""
        
        for _, row in sector_stats.iterrows():
            report += f"- **{row['business_sector']}**: {row['count']} æ¡ (å¹³å‡åˆ†æ•°: {row['avg_score']:.2f})\n"
        
        report += f"""
## ğŸ¯ é‡ç‚¹å•†ä¸šæœºä¼š

"""
        
        for i, (_, opp) in enumerate(opportunities.head(10).iterrows(), 1):
            report += f"""### {i}. {opp['description']}
- **ç—›ç‚¹ç±»åˆ«**: {opp['category']}
- **å‡ºç°é¢‘æ¬¡**: {opp['frequency']} æ¬¡
- **æœºä¼šåˆ†æ•°**: {opp['avg_opportunity_score']:.2f}/10
- **å¸‚åœºè§„æ¨¡**: {opp['market_size_estimate']}
- **è§£å†³æ–¹æ¡ˆ**: {opp['potential_solution']}

"""
        
        report += f"""
## ğŸ’¡ è¡ŒåŠ¨å»ºè®®

### ä¼˜å…ˆçº§æ’åº
1. **é«˜é¢‘é«˜åˆ†ç—›ç‚¹** - é‡ç‚¹å…³æ³¨å‡ºç°é¢‘æ¬¡>10ä¸”åˆ†æ•°>7çš„æœºä¼š
2. **æŠ€æœ¯å¯è¡Œæ€§** - è¯„ä¼°è§£å†³æ–¹æ¡ˆçš„æŠ€æœ¯å®ç°éš¾åº¦
3. **å¸‚åœºè§„æ¨¡** - ä¼˜å…ˆé€‰æ‹©ä¸­å¤§å‹å¸‚åœºæœºä¼š

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨
1. **æ·±åº¦è°ƒç ”** - å¯¹TOP3å•†æœºè¿›è¡Œè¯¦ç»†å¸‚åœºè°ƒç ”
2. **åŸå‹å¼€å‘** - å¿«é€Ÿå¼€å‘MVPéªŒè¯è§£å†³æ–¹æ¡ˆ
3. **ç”¨æˆ·éªŒè¯** - ä¸ç›®æ ‡ç”¨æˆ·æ·±åº¦è®¿è°ˆéªŒè¯éœ€æ±‚

## ğŸ“Š æ•°æ®æ–‡ä»¶
- `process_complaints.csv` - åŸå§‹æŠ±æ€¨æ•°æ®
- `business_opportunities.csv` - å•†æœºåˆ†ææ•°æ®

---
*æœ¬æŠ¥å‘Šç”±æµç¨‹ä¼˜åŒ–å•†æœºå‘ç°å·¥å…·è‡ªåŠ¨ç”Ÿæˆ*
"""
        
        # ä¿å­˜æŠ¥å‘Š
        output_dir = 'process_optimization_output'
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        with open(f'{output_dir}/opportunity_analysis_report.md', 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info("åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        return report
    
    def run_full_analysis(self, search_terms, max_tweets_per_term=20):
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        try:
            # è®¾ç½®æµè§ˆå™¨
            if not self.setup_chrome_driver(headless=True):
                logger.error("æµè§ˆå™¨è®¾ç½®å¤±è´¥")
                return
            
            # æœç´¢æŠ±æ€¨
            complaints = self.search_process_complaints(search_terms, max_tweets_per_term)
            
            if complaints:
                # ä¿å­˜æ•°æ®
                self.save_complaints(complaints)
                
                # åˆ†æå•†æœº
                opportunities = self.analyze_business_opportunities()
                
                # å¯¼å‡ºç»“æœ
                self.export_results()
                
                # ç”ŸæˆæŠ¥å‘Š
                report = self.generate_report()
                
                print(f"\nğŸ‰ åˆ†æå®Œæˆï¼")
                print(f"ğŸ“Š å…±æŠ“å– {len(complaints)} æ¡æµç¨‹ç›¸å…³æŠ±æ€¨")
                print(f"ğŸ¯ è¯†åˆ«å‡º {len(opportunities)} ä¸ªå•†ä¸šæœºä¼š")
                print(f"ğŸ“ ç»“æœä¿å­˜åœ¨ process_optimization_output ç›®å½•")
                
                return opportunities
            else:
                logger.warning("æœªæ‰¾åˆ°ç›¸å…³æ•°æ®")
                return []
                
        except Exception as e:
            logger.error(f"åˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
            return []
        finally:
            if self.driver:
                self.driver.quit()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ æµç¨‹ä¼˜åŒ–å•†æœºå‘ç°å·¥å…·")
    print("=" * 50)
    
    # æµç¨‹ä¼˜åŒ–ç›¸å…³æœç´¢è¯
    search_terms = [
        "é“¶è¡ŒåŠäº‹å¤ªéº»çƒ¦",
        "æ”¿åŠ¡æœåŠ¡æµç¨‹å¤æ‚",
        "åŒ»é™¢æŒ‚å·å¤ªå¤æ‚",
        "å¿«é€’æ”¶å‘æµç¨‹",
        "é€€è´§æµç¨‹ç¹ç",
        "å®¢æœæµç¨‹ä½“éªŒå·®",
        "banking process complicated",
        "government service process",
        "hospital appointment process",
        "return process frustrating"
    ]
    
    scraper = ProcessOptimizationScraper()
    opportunities = scraper.run_full_analysis(search_terms, max_tweets_per_term=15)
    
    if opportunities:
        print("\nğŸ† TOP 5 å•†ä¸šæœºä¼š:")
        for i, opp in enumerate(opportunities[:5], 1):
            print(f"{i}. {opp['description']} (åˆ†æ•°: {opp['avg_opportunity_score']:.1f})")

if __name__ == "__main__":
    main()